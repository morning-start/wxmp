# 使用指南

## 快速开始

### 1. 安装

```bash
pip install wxmp
```

或从源码安装：

```bash
git clone <repository-url>
cd wxmp
pip install -e .
```

### 2. 获取 Cookies

1. 在浏览器中打开 [微信公众平台](https://mp.weixin.qq.com/)
2. 登录你的公众号账号
3. 打开浏览器开发者工具（F12）
4. 切换到 Network 标签
5. 刷新页面
6. 找到任意请求，查看 Request Headers 中的 Cookie
7. 复制完整的 Cookie 字符串

**示例 Cookie 格式**:

```
wxuin=123456789; pass_ticket=abcdefg; ...
```

### 3. 保存 Cookies

将 cookies 保存为 JSON 文件：

```json
{
  "请求 Cookie": {
    "wxuin": "your_wxuin",
    "pass_ticket": "your_pass_ticket",
    ...
  }
}
```

---

## 使用场景

### 场景 1: 直接使用 API

适合需要精细控制 API 调用的场景。

```python
from wxmp import WxMPAPI

# 初始化 API
cookies = {
    "wxuin": "your_wxuin",
    "pass_ticket": "your_pass_ticket",
}

api = WxMPAPI(cookies)

# 搜索公众号
response = api.fetch_fakeid("Python")
print(f"找到 {response.total} 个结果")

for account in response.list:
    print(f"名称: {account.nickname}")
    print(f"FakeID: {account.fakeid}")
```

---

### 场景 2: 使用已有爬虫（推荐）

适合快速获取公众号文章的场景，支持缓存优化。

#### 2.1 初始化爬虫

```python
from wxmp.spider import TimeRangeSpider

# 从 Cookie 文件初始化
spider = TimeRangeSpider.from_cookies_file("cookies.json")

# 或直接传入 cookies
cookies = {...}
spider = TimeRangeSpider(cookies)
```

#### 2.2 搜索公众号

```python
# 搜索公众号（带缓存）
bizs = spider.load_or_search_bizs(
    gzh_names=["Python编程", "机器学习"],
    cache_file=Path("temp/fakeids.json")
)

print(bizs)
# 输出: {"Python编程": "MzI...", "机器学习": "MzI..."}
```

#### 2.3 获取文章列表

```python
from datetime import datetime
from wxmp.spider import TimeRange

# 定义时间范围
time_range = TimeRange(
    begin=datetime(2024, 1, 1),
    end=datetime(2024, 12, 31)
)

# 获取文章列表（带时间范围缓存）
df = spider.search_articles_content(
    bizs={"Python编程": "MzI..."},
    time_range=time_range,
    save_dir=Path("temp/articles_info/")
)

print(df.head())
```

#### 2.4 下载文章内容

```python
# 下载所有文章内容
spider.save_all_article_content(
    df=df,
    save_dir=Path("temp/article_content/"),
    max_workers=5,
    time_range=time_range,
    save_file="md",  # 或 "html"
    min_file_size_kb=3
)
```

---

### 场景 3: 自定义爬虫

适合需要自定义业务逻辑的场景。

#### 3.1 继承 TimeRangeSpider

```python
from wxmp.spider import TimeRangeSpider
from wxmp import WxMPAPI

class CustomSpider(TimeRangeSpider):
    def custom_search_logic(self, keywords: list[str]):
        """自定义搜索逻辑"""
        results = []
        for keyword in keywords:
            response = self.fetch_fakeid(keyword)
            results.extend(response.list)
        return results

    def custom_filter_articles(self, articles, min_likes: int = 100):
        """自定义文章过滤"""
        return [a for a in articles if a.read_num >= min_likes]

# 使用自定义爬虫
spider = CustomSpider(cookies)
results = spider.custom_search_logic(["Python", "Java"])
```

#### 3.2 继承 WxMPAPI

```python
from wxmp import WxMPAPI

class ExtendedAPI(WxMPAPI):
    def custom_api_call(self, custom_params):
        """自定义 API 调用"""
        url = self.domain + "/custom/endpoint"
        params = {...}
        res = self.session.get(url, params=params, headers=self.headers)
        return res.json()

# 使用扩展 API
api = ExtendedAPI(cookies)
result = api.custom_api_call({...})
```

---

## 高级用法

### 1. 缓存管理

#### 1.1 FakeID 缓存

```python
from pathlib import Path

# 加载或搜索 fakeid（自动缓存）
bizs = spider.load_or_search_bizs(
    gzh_names=["Python编程"],
    cache_file=Path("temp/fakeids.json")
)

# 缓存文件格式
# temp/fakeids.json
{
    "Python编程": "MzI...",
    "机器学习": "MzI..."
}
```

#### 1.2 时间范围缓存

```python
# 时间范围缓存自动管理
# 缓存文件: temp/articles_info/{公众号名}.json

# 第一次运行：获取 2024-01-01 到 2024-06-30 的文章
time_range = TimeRange(begin=datetime(2024, 1, 1), end=datetime(2024, 6, 30))
df1 = spider.search_articles_content(bizs, time_range)

# 第二次运行：只获取 2024-07-01 到 2024-12-31 的文章
time_range = TimeRange(begin=datetime(2024, 1, 1), end=datetime(2024, 12, 31))
df2 = spider.search_articles_content(bizs, time_range)

# 缓存文件自动更新
```

### 2. 并发控制

```python
# 调整并发数
spider.save_all_article_content(
    df=df,
    save_dir=Path("temp/article_content/"),
    max_workers=10,  # 增加并发数
    time_range=time_range
)
```

### 3. 错误处理

```python
from wxmp import WxMPAPI
from wxmp.api import TokenError, SearchBizError, ListExError

try:
    api = WxMPAPI(cookies)
    response = api.fetch_fakeid("Python")
except TokenError as e:
    print(f"Token 获取失败: {e}")
except SearchBizError as e:
    print(f"搜索公众号失败: {e}")
except ListExError as e:
    print(f"获取文章列表失败: {e}")
except Exception as e:
    print(f"未知错误: {e}")
```

### 4. 数据处理

```python
import pandas as pd

# 读取文章数据
df = pd.read_csv("temp/articles_info/Python编程.csv")

# 按时间排序
df["create_time"] = pd.to_datetime(df["create_time"])
df = df.sort_values("create_time", ascending=False)

# 筛选文章
filtered_df = df[df["create_time"] >= datetime(2024, 1, 1)]

# 导出数据
filtered_df.to_csv("filtered_articles.csv", index=False)
```

### 5. 文章内容处理

```python
from wxmp.tools import save_article_content, sanitize_filename
from pathlib import Path

# 保存文章内容
html = "<html>...</html>"
save_path = Path("temp/articles/my_article.md")

save_article_content(
    html=html,
    save_path=save_path,
    save_file="md",
    title="文章标题",
    date_str="2024-01-01",
    link="https://mp.weixin.qq.com/s/xxx",
    account_name="公众号名",
    digest="文章摘要",
    min_file_size_kb=3
)
```

---

## 完整示例

### 示例 1: 批量下载公众号文章

```python
from pathlib import Path
from datetime import datetime
from wxmp.spider import TimeRangeSpider, TimeRange

# 1. 初始化爬虫
spider = TimeRangeSpider.from_cookies_file("cookies.json")

# 2. 搜索公众号
gzh_names = ["Python编程", "机器学习", "数据分析"]
bizs = spider.load_or_search_bizs(gzh_names)

# 3. 定义时间范围
time_range = TimeRange(
    begin=datetime(2024, 1, 1),
    end=datetime(2024, 12, 31)
)

# 4. 获取文章列表
df = spider.search_articles_content(
    bizs=bizs,
    time_range=time_range,
    save_dir=Path("temp/articles_info/")
)

print(f"共获取 {len(df)} 篇文章")

# 5. 下载文章内容
spider.save_all_article_content(
    df=df,
    save_dir=Path("temp/article_content/"),
    max_workers=5,
    time_range=time_range,
    save_file="md"
)

print("下载完成！")
```

---

### 示例 2: 增量更新文章

```python
from pathlib import Path
from datetime import datetime, timedelta
from wxmp.spider import TimeRangeSpider, TimeRange

# 1. 初始化爬虫
spider = TimeRangeSpider.from_cookies_file("cookies.json")

# 2. 搜索公众号
bizs = spider.load_or_search_bizs(["Python编程"])

# 3. 定义时间范围（最近 7 天）
end_date = datetime.today()
begin_date = end_date - timedelta(days=7)
time_range = TimeRange(begin=begin_date, end=end_date)

# 4. 增量获取文章（自动使用缓存）
df = spider.search_articles_content(
    bizs=bizs,
    time_range=time_range,
    save_dir=Path("temp/articles_info/")
)

# 5. 下载新文章
if not df.empty:
    spider.save_all_article_content(
        df=df,
        save_dir=Path("temp/article_content/"),
        time_range=time_range
    )
    print(f"更新了 {len(df)} 篇文章")
else:
    print("没有新文章")
```

---

### 示例 3: 自定义过滤和导出

```python
from pathlib import Path
from datetime import datetime
import pandas as pd
from wxmp.spider import TimeRangeSpider, TimeRange

# 1. 获取文章列表
spider = TimeRangeSpider.from_cookies_file("cookies.json")
bizs = spider.load_or_search_bizs(["Python编程"])
time_range = TimeRange(begin=datetime(2024, 1, 1))
df = spider.search_articles_content(bizs, time_range)

# 2. 自定义过滤
df["create_time"] = pd.to_datetime(df["create_time"])

# 过滤条件
filtered_df = df[
    (df["create_time"] >= datetime(2024, 6, 1)) &
    (df["create_time"] <= datetime(2024, 6, 30))
]

# 3. 导出为不同格式
filtered_df.to_csv("june_articles.csv", index=False, encoding="utf-8-sig")
filtered_df.to_excel("june_articles.xlsx", index=False)
filtered_df.to_json("june_articles.json", orient="records", force_ascii=False)

# 4. 下载文章内容
spider.save_all_article_content(
    df=filtered_df,
    save_dir=Path("temp/june_articles/"),
    time_range=time_range
)
```

---

## 最佳实践

### 1. Cookie 管理

- 不要将 Cookie 硬编码在代码中
- 使用环境变量或配置文件
- 定期更新 Cookie

```python
import os
from wxmp import WxMPAPI

# 从环境变量读取
cookies = {
    "wxuin": os.getenv("WXUIN"),
    "pass_ticket": os.getenv("PASS_TICKET"),
}

api = WxMPAPI(cookies)
```

### 2. 错误处理

- 捕获所有可能的异常
- 记录错误日志
- 提供友好的错误提示

```python
from loguru import logger

try:
    df = spider.search_articles_content(bizs, time_range)
except Exception as e:
    logger.error(f"获取文章失败: {e}")
    raise
```

### 3. 频率控制

- 合理设置请求间隔
- 避免触发微信的频率限制

```python
import time

for i, article in enumerate(articles):
    # 处理文章
    process_article(article)

    # 每处理 10 篇文章，休息 1 秒
    if i % 10 == 0:
        time.sleep(1)
```

### 4. 数据备份

- 定期备份缓存文件
- 保存原始数据

```python
import shutil
from datetime import datetime

# 备份缓存
backup_dir = Path(f"backup/{datetime.now().strftime('%Y%m%d')}")
backup_dir.mkdir(parents=True, exist_ok=True)

shutil.copy("temp/fakeids.json", backup_dir / "fakeids.json")
shutil.copytree("temp/articles_info", backup_dir / "articles_info")
```

---

## 常见问题

### Q1: Token 获取失败怎么办？

A: 检查 Cookie 是否有效，尝试重新登录微信公众平台获取新的 Cookie。

### Q2: 如何处理文章已失效的情况？

A: 使用 `is_valid_article_link` 方法检查链接有效性。

```python
if api.is_valid_article_link(article.link):
    html = api.fetch_article_content(article.link)
else:
    print("文章已失效")
```

### Q3: 如何提高下载速度？

A: 增加 `max_workers` 参数，但不要设置过大（建议 5-10）。

### Q4: 缓存文件在哪里？

A:

- FakeID 缓存: `temp/fakeids.json`
- 时间范围缓存: `temp/articles_info/{公众号名}.json`
- 文章列表: `temp/articles_info/{公众号名}.csv`
- 文章内容: `temp/article_content/{公众号名}/`

### Q5: 如何清空缓存？

A: 删除 `temp/` 目录下的缓存文件。

```python
import shutil

shutil.rmtree("temp")
```

---

## 相关文档

- [项目概览](./项目概览.md)
- [架构设计](./架构设计.md)
- [API 文档](./API文档.md)
- [数据流动与状态管理](./数据流动与状态管理.md)
